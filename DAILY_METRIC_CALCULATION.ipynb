{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEVLXJ7gkyrspMLIQg7T0j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuinnG17/Financial-Programs/blob/main/DAILY_METRIC_CALCULATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNcjlbjKdgWK"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import hashlib\n",
        "import time\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "\n",
        "spark_df = spark.sql(\"SELECT * FROM `hive_metastore`.`alpha_two`.`all_daily_returns`\")\n",
        "\n",
        "previous_metrics = spark.sql(\"SELECT * FROM `hive_metastore`.`alpha_two`.`all_date_metrics`\")\n",
        "past_metrics = previous_metrics.toPandas()\n",
        "past_metrics['Date'] = pd.to_datetime(past_metrics['Date'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark_df.toPandas()\n",
        "df = df.set_index('Date')\n",
        "df = df.sort_index(ascending=True)\n",
        "past_metrics = past_metrics.sort_values('Date')\n",
        "\n",
        "df = df.div(100)\n",
        "\n",
        "metrics_dict = {}\n",
        "all_output = pd.DataFrame({})\n",
        "\n",
        "for symbol in df.columns:\n",
        "\n",
        "    symbol = symbol\n",
        "\n",
        "    if symbol not in metrics_dict.keys():\n",
        "        metrics_dict[symbol] = {}\n",
        "\n",
        "    start_date = past_metrics['Date'].iloc[-1]\n",
        "\n",
        "    iterations = df[df.index > start_date]\n",
        "\n",
        "    for date in iterations.index:\n",
        "        #print(f\"{symbol} at {date}\")\n",
        "\n",
        "        time_period = df[df.index < date]\n",
        "\n",
        "        daily_returns = time_period[symbol]\n",
        "        #print(daily_returns)\n",
        "\n",
        "        sharpe_ratio = (252 ** 0.5) * (daily_returns.mean() / daily_returns.std())\n",
        "\n",
        "        # calculate the Net Profit %\n",
        "        returns = daily_returns.add(1)\n",
        "\n",
        "        cumprod = returns.cumprod()\n",
        "\n",
        "        net_profit_pct = (cumprod.iloc[-1] - 1)*100\n",
        "        #print(net_profit_pct)\n",
        "\n",
        "        # calculate the Drawdown %\n",
        "        equity_curve = cumprod * 100000\n",
        "        #rint(equity_curve)\n",
        "        previous_peaks=equity_curve.cummax()\n",
        "        drawdowns = (equity_curve - previous_peaks) / previous_peaks\n",
        "        drawdown = drawdowns.min() * 100\n",
        "\n",
        "\n",
        "        # calculate the Total Trades\n",
        "\n",
        "        trades = daily_returns[daily_returns != 0]\n",
        "\n",
        "        trades = len(trades)\n",
        "\n",
        "        # calculate the Average Win and Average Loss\n",
        "        gains = daily_returns[daily_returns > 0]\n",
        "        #rint(len(gains))\n",
        "        losses = daily_returns[daily_returns < 0]\n",
        "        #rint(len(losses))\n",
        "        average_win = gains.mean()\n",
        "        average_loss = losses.mean() * -1\n",
        "\n",
        "        # calculate the Alpha\n",
        "        risk_free_rate = 0.02 / 365\n",
        "        benchmark_daily_returns = time_period\n",
        "        benchmark_returns = benchmark_daily_returns['HLT'] #Should be SPY but we don't have data for that\n",
        "        #benchmark_returns = benchmark_returns.loc[daily_returns.index]\n",
        "        excess_returns = (daily_returns - benchmark_returns - risk_free_rate)\n",
        "        #rint(excess_returns)\n",
        "        beta, alpha = np.polyfit(benchmark_returns, excess_returns, 1)\n",
        "\n",
        "        # calculate the Compounding Annual Return\n",
        "        days = (time_period.index[-1] - time_period.index[0]).days\n",
        "        compound_annual_return = ((1 + (net_profit_pct / 100)) ** (365.25 / days)) - 1\n",
        "\n",
        "        # calculate the Expectancy\n",
        "        if trades > 0:\n",
        "            win_rate = (len(gains) / trades) * 100\n",
        "            loss_rate = (len(losses) / trades) * 100\n",
        "            expectancy = (average_win * win_rate / 100) - (average_loss * loss_rate / 100)\n",
        "        else:\n",
        "            win_rate = 0\n",
        "            loss_rate = 0\n",
        "            expectancy = 0\n",
        "\n",
        "        # calculate the Beta\n",
        "        beta = excess_returns.cov(benchmark_returns) / benchmark_returns.var()\n",
        "\n",
        "        # calculate the Loss Rate\n",
        "        loss_rate = (losses.count() / trades) * 100\n",
        "\n",
        "        # calculate the Profit-Loss Ratio\n",
        "        profit_loss_ratio = gains.sum() / losses.abs().sum()\n",
        "\n",
        "        # calculate the Annual Standard Deviation and Annual Variance\n",
        "        annual_std_dev = daily_returns.std() * (252 ** 0.5)\n",
        "        annual_var = annual_std_dev ** 2\n",
        "\n",
        "        # calculate the Information Ratio and Tracking Error\n",
        "        if symbol =='HLT':\n",
        "            information_ratio = 0\n",
        "        else:\n",
        "            information_ratio = excess_returns.mean() / excess_returns.std()\n",
        "\n",
        "        tracking_error = excess_returns.std()\n",
        "\n",
        "        # calculate the Total Fees\n",
        "        total_trades = trades\n",
        "        commission_per_trade = 5.00\n",
        "        total_fees = total_trades * commission_per_trade\n",
        "\n",
        "        # calculate the Treynor Ratio\n",
        "        # calculate the Treynor Ratio\n",
        "        beta, alpha = np.polyfit(benchmark_returns, excess_returns, 1)\n",
        "        risk_free_rate = 0.02\n",
        "        portfolio_returns = daily_returns.mean()\n",
        "        treynor_ratio = (portfolio_returns - risk_free_rate) / beta\n",
        "\n",
        "\n",
        "        metrics_dict[symbol][date] = {\n",
        "                'Symbol' : symbol,\n",
        "                'Date': date,\n",
        "                'Sharpe_Ratio': float(sharpe_ratio),\n",
        "                'Net_Profit': float(net_profit_pct),\n",
        "                'Drawdown': float(drawdown),\n",
        "                'Total_Trades': float(trades),\n",
        "                'Average_Win': float(average_win),\n",
        "                'Average_Loss': float(average_loss),\n",
        "                'Alpha': float(alpha),\n",
        "                'Compounding_Annual_Return': float(compound_annual_return * 100),\n",
        "                'Expectancy': float(expectancy),\n",
        "                'Beta': float(beta),\n",
        "                'Loss_Rate': float(loss_rate),\n",
        "                'Win_Rate': float(win_rate),\n",
        "                'Profit_Loss_Ratio': float(profit_loss_ratio),\n",
        "                'Annual_Standard_Deviation': float(annual_std_dev),\n",
        "                'Annual_Variance': float(annual_var),\n",
        "                'Information_Ratio': float(information_ratio),\n",
        "                'Tracking_Error': float(tracking_error),\n",
        "                'Total_Fees': float(total_fees),\n",
        "                'Treynor_Ratio': float(treynor_ratio)\n",
        "            }\n",
        "\n",
        "    print(len(metrics_dict.keys()) / len(df.columns))"
      ],
      "metadata": {
        "id": "dtnwKmgmdhPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_output = pd.DataFrame.from_dict(metrics_dict['HLT'])\n",
        "\n",
        "all_output = all_output.T\n",
        "\n",
        "for key in metrics_dict.keys():\n",
        "    symbol_output = pd.DataFrame.from_dict(metrics_dict[key])\n",
        "    symbol_output = symbol_output.T\n",
        "    all_output = pd.concat([all_output, symbol_output])\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "output = all_output\n",
        "\n",
        "cols_to_convert = output.columns[2:]  # Get all column names except the first 2 columns\n",
        "output[cols_to_convert] = output[cols_to_convert].astype(float)\n",
        "output['Symbol'] = output['Symbol'].str.strip()\n",
        "output['Symbol'] = output['Symbol'].astype(str)\n",
        "\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "WfY-fgVWdimv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([past_metrics, output])\n",
        "all_data = all_data.sort_values(by=['Symbol', 'Date'])\n",
        "print(all_data)"
      ],
      "metadata": {
        "id": "F7avvlyEdkdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName('pandas-to-databricks').getOrCreate()\n",
        "\n",
        "# Convert the Pandas DataFrame to a Databricks DataFrame\n",
        "db_df = spark.createDataFrame(all_data)\n",
        "\n",
        "db_df.createOrReplaceTempView('final_output')\n",
        "\n",
        "db_df.show()"
      ],
      "metadata": {
        "id": "Fr8stNmNdl63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql create or replace table alpha_two.ALL_DATE_METRICS_BACKUP as select * from alpha_two.ALL_DATE_METRICS"
      ],
      "metadata": {
        "id": "a9DzK8r-dnHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql create or replace table alpha_two.ALL_DATE_METRICS as select * from final_output"
      ],
      "metadata": {
        "id": "ahepfCNUdo4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}