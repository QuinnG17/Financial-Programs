{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/uwLxfb/L2uTKajWrRR8p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jo1Fq7OYZmX"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "Yixu6djSYoB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Get a list of all equity tickers\n",
        "equities = pd.read_csv('http://ftp.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt', delimiter='|')\n",
        "equities = equities[equities['Test Issue'] == 'N']  # Exclude test issues\n",
        "\n",
        "ticker_list = equities['NASDAQ Symbol'].tolist()\n",
        "\n",
        "string_list = [str(item) for item in ticker_list]\n",
        "\n",
        "print(string_list)"
      ],
      "metadata": {
        "id": "P8f7EFcTYpgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch the historical daily data for the symbols\n",
        "data = yf.download(string_list, start='2021-01-01', end='2023-05-31')\n",
        "\n",
        "yahoo_data = spark.createDataFrame(data)\n",
        "\n",
        "# Print the retrieved data\n",
        "print(data)"
      ],
      "metadata": {
        "id": "zvc6dNPCYrCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yahoo_data = spark.createDataFrame(data)\n"
      ],
      "metadata": {
        "id": "xf6x7itWYs3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n",
        "data.index = pd.to_datetime(data.index)\n",
        "signals = spark.read.table('kash.signal')\n",
        "\n",
        "signals = signals.toPandas()\n",
        "\n",
        "signals['Date'] = pd.to_datetime(signals['Date'])\n",
        "\n",
        "df = data\n",
        "df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
        "\n",
        "# Transpose the second level of the multi-index column onto the row index\n",
        "df = df.stack(level=1)\n",
        "\n",
        "df['Symbol'] = df.index.get_level_values(1)\n",
        "\n",
        "all_data = df.merge(signals, on='Date', how='inner')\n",
        "\n",
        "all_data['Signal'] = all_data['Signal'].replace(2, -1)\n",
        "# Print the updated DataFrame\n",
        "print(all_data)"
      ],
      "metadata": {
        "id": "yncK0TZAYuh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['Change'] = (all_data['Close'] - all_data['Open']) / all_data['Open']\n",
        "\n",
        "all_data['Return'] = (all_data['Change'] * all_data['Signal']) + 1\n",
        "\n",
        "\n",
        "print(all_data)"
      ],
      "metadata": {
        "id": "uKVK_RPrYxjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily_return_data = all_data[['Date', 'Symbol', 'Return']]\n",
        "daily_return_data['Return'] = (daily_return_data['Return'] - 1) * 100\n",
        "\n",
        "daily_return_data.set_index('Date', inplace=True)\n",
        "\n",
        "df_pivoted = daily_return_data.pivot(columns='Symbol', values='Return')\n",
        "\n",
        "df_pivoted = df_pivoted.reset_index()\n",
        "\n",
        "db_df = spark.createDataFrame(df_pivoted)\n",
        "\n",
        "db_df.createOrReplaceTempView('results')\n",
        "\n",
        "print(df_pivoted)"
      ],
      "metadata": {
        "id": "c7kVt7GfYzYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql CREATE OR REPLACE TABLE alpha_two.YAHOO_ALL_DAILY_RETURNS as select * from results"
      ],
      "metadata": {
        "id": "iV5E9RiOY11g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data['Hold_Return'] = all_data['Change'] + 1\n",
        "\n",
        "all_data = all_data.sort_values('Date')\n",
        "\n",
        "all_data['Cumulative_Return'] = all_data.groupby('Symbol')['Return'].cumprod()\n",
        "all_data['Cumulative_Hold'] = all_data.groupby('Symbol')['Hold_Return'].cumprod()\n",
        "\n",
        "all_data['ExcessReturn'] = all_data['Cumulative_Return'] - all_data['Cumulative_Hold']\n",
        "\n",
        "info = all_data[['Date', 'Symbol', 'Cumulative_Return', 'Cumulative_Hold', 'ExcessReturn']]\n",
        "\n",
        "print(info)"
      ],
      "metadata": {
        "id": "FcNrybsgY2W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_df = spark.createDataFrame(info)\n",
        "\n",
        "db_df.createOrReplaceTempView('results')"
      ],
      "metadata": {
        "id": "aQx6CH75Y4A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql CREATE OR REPLACE TABLE alpha_two.YAHOO_ALL_DAILY_EXCESS_RETURNS as select * from results"
      ],
      "metadata": {
        "id": "LlvAlXR5Y5rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def get_stock_details(ticker):\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        return {\n",
        "            'Ticker': ticker,\n",
        "            'Industry': stock.info.get('industry'),\n",
        "            'Sector': stock.info.get('sector'),\n",
        "            'MarketCap': stock.info.get('marketCap'),\n",
        "            'Market': stock.info.get('exchange')\n",
        "        }\n",
        "    except Exception:\n",
        "        print(\"Exception\")\n",
        "        return {\n",
        "            'Ticker': ticker,\n",
        "            'Industry': None,\n",
        "            'Sector': None,\n",
        "            'MarketCap': None,\n",
        "            'Market': None\n",
        "        }\n",
        "\n",
        "tqdm.pandas()  # Enable progress bar for pandas apply\n",
        "unique_values = info['Symbol'].unique()\n",
        "\n",
        "unique_df = pd.DataFrame({'Symbol': unique_values})\n",
        "\n",
        "unique_df['Symbol'] = unique_df['Symbol'].progress_apply(get_stock_details)\n",
        "\n",
        "details_df = unique_df['Symbol'].progress_apply(pd.Series)\n",
        "\n",
        "merged_df = pd.concat([unique_df, details_df], axis=1)"
      ],
      "metadata": {
        "id": "gsWkcPXQY66k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all = pd.merge(info, merged_df[['Ticker', 'Industry', 'Sector', 'MarketCap', 'Market']], left_on='Symbol', right_on='Ticker', how='inner')\n",
        "\n",
        "print(all)"
      ],
      "metadata": {
        "id": "9xwt3IGrY-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_df = spark.createDataFrame(all)\n",
        "\n",
        "db_df.createOrReplaceTempView('results')"
      ],
      "metadata": {
        "id": "khkVRvKCZBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql CREATE OR REPLACE TABLE alpha_two.ALL_SIGNAL_RETURNS_WITH_FUNDAMENTALS as select * from results"
      ],
      "metadata": {
        "id": "TEvpk4inZB_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Only needed if column names need to be cleaned\n",
        "\n",
        "import re\n",
        "# Function to remove invalid characters and spaces from column names\n",
        "def clean_column_name(column_name):\n",
        "    # Remove invalid characters using regular expression pattern\n",
        "    cleaned_name = re.sub(r'[^a-zA-Z0-9_]', '', column_name)\n",
        "    # Remove leading and trailing spaces\n",
        "    cleaned_name = cleaned_name.strip()\n",
        "    return cleaned_name\n",
        "\n",
        "# Clean column names using the clean_column_name function\n",
        "all_data.columns = all_data.columns.map(clean_column_name)\n",
        "\n",
        "# Print the DataFrame with cleaned column names\n",
        "print(all_data)\n",
        "\n",
        "db_df = spark.createDataFrame(all_data)\n",
        "\n",
        "db_df.createOrReplaceTempView('results')"
      ],
      "metadata": {
        "id": "tnP5EI-XZDKv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}