{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwXo/In7cHDT4iufS4YF5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuinnG17/Financial-Programs/blob/main/LINEAR_REGRESSION_MODEL_FITTING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epgRfi8FZO_t"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Read long signal return information\n",
        "long_df = spark.read.table(\"alpha_two.all_date_long_metrics_v2\")\n",
        "long_data = long_df.toPandas()\n",
        "long_data['Date'] = pd.to_datetime(long_data['Date'])\n",
        "\n",
        "# Read short signal return information\n",
        "short_df = spark.read.table(\"alpha_two.all_date_short_metrics_v2\")\n",
        "short_data = short_df.toPandas()\n",
        "short_data['Date'] = pd.to_datetime(short_data['Date'])\n",
        "\n",
        "#read Signals\n",
        "table_df = spark.read.table(\"kash.signal_shifted\")\n",
        "signals = table_df.toPandas()\n",
        "signals['Date'] = pd.to_datetime(signals['Date'])\n",
        "\n",
        "#Concat data for Target column calculation\n",
        "all_data = pd.concat([long_data, short_data])\n",
        "data = pd.merge(all_data, signals, on='Date', how='inner')\n",
        "data['Signal'] = data['Signal'].replace(2, -1)\n",
        "\n",
        "data = data.sort_values(by='Date')\n",
        "\n",
        "# Subtract the next row by the current row within each group\n",
        "data['NextValue'] = data.groupby('Symbol')['Net_Profit'].shift(-1)\n",
        "data['Target'] = data['NextValue'] - data['Net_Profit']\n",
        "\n",
        "print(long_data.columns)\n",
        "print(data.columns)\n",
        "\n",
        "#Separate Target Values back out\n",
        "long_data = long_data.merge(data[['Symbol', 'Date', 'NextValue', 'Signal', 'Target']], on=['Date', 'Symbol'], how='inner')\n",
        "short_data = short_data.merge(data[['Symbol', 'Date', 'NextValue', 'Signal', 'Target']], on=['Date','Symbol'], how='inner')\n",
        "\n",
        "print(len(long_data['Date'].unique()))\n",
        "print(len(short_data['Date'].unique()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import col, current_date, date_sub, lit, when\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.sql.functions import desc\n",
        "from pyspark.sql.functions import col, date_sub, expr\n",
        "from pyspark.sql.types import DateType\n",
        "from datetime import timedelta\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load your daily data into a DataFrame\n",
        "data = spark.createDataFrame(long_data)\n",
        "data = data.withColumn('Date', col('Date').cast(DateType()))\n",
        "data = data.orderBy('Date')\n",
        "\n",
        "\n",
        "# Drop multiple columns\n",
        "columns_to_drop = ['Date', 'Symbol', 'Target', 'NextValue', 'Target']\n",
        "columns = data.drop(*columns_to_drop)\n",
        "feature_columns = columns.columns\n",
        "\n",
        "distinct_dates = data.select(\"Date\").distinct().orderBy(\"Date\")\n",
        "distinct_dates_list = [row.Date for row in distinct_dates.collect()]\n",
        "\n",
        "# Create an empty DataFrame with the specified columns\n",
        "extra_columns = ['Symbol', 'Date', 'Model', 'Direction','Accuracy']\n",
        "\n",
        "all_columns = extra_columns + feature_columns\n",
        "\n",
        "weights_df = pd.DataFrame(columns=all_columns)\n",
        "\n",
        "for date in distinct_dates_list:\n",
        "    print(date)\n",
        "\n",
        "    start_date = date - timedelta(days=60)\n",
        "\n",
        "    if start_date <= distinct_dates_list[0]:\n",
        "        continue\n",
        "\n",
        "    # Define the model\n",
        "    model = LinearRegression(featuresCol='features', labelCol='Target')\n",
        "\n",
        "    # Fit models for each symbol\n",
        "    distinct_symbols = data.select('Symbol').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    # Define the features column\n",
        "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "\n",
        "    for symbol in distinct_symbols:\n",
        "\n",
        "        # Filter for the last 60 days and specific symbols\n",
        "        symbol_data = data.filter((col('Date') >= start_date) & (col('Symbol') == symbol))\n",
        "        symbol_data = symbol_data.na.drop()\n",
        "\n",
        "        if symbol_data.isEmpty():\n",
        "            continue\n",
        "\n",
        "        # Split the data into training and test sets (70% training, 30% test)\n",
        "        trainData, testData = symbol_data.randomSplit([0.7, 0.3], seed=123)\n",
        "\n",
        "        trainData = assembler.transform(trainData)\n",
        "        testData = assembler.transform(testData)\n",
        "\n",
        "        # Fit model for the symbol\n",
        "        trained_model = model.fit(trainData)\n",
        "\n",
        "        # Get the feature weights from the trained model\n",
        "        weights = trained_model.coefficients\n",
        "\n",
        "        # Make predictions on a test dataset\n",
        "        predictions = trained_model.transform(testData)\n",
        "\n",
        "        # Create a MulticlassClassificationEvaluator\n",
        "        #evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        #accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "        # Add a new column 'accuracy' to indicate if both prediction and target are above or below 0\n",
        "        predictions = predictions.withColumn('accuracy', when((col('target') > 0) & (col('prediction') > 0), 1)\n",
        "                                                        .when((col('target') < 0) & (col('prediction') < 0), 1)\n",
        "                                                        .otherwise(0))\n",
        "\n",
        "        # Calculate the accuracy percentage\n",
        "        accuracy_percentage = predictions.selectExpr('avg(accuracy) * 100 as accuracy_percentage').first()['accuracy_percentage']\n",
        "\n",
        "        other_data = [symbol, date, model.__class__.__name__, \"Long\", accuracy_percentage]\n",
        "\n",
        "        other_data.extend(weights)\n",
        "\n",
        "        weights_df = weights_df.append(pd.Series(other_data, index=weights_df.columns), ignore_index=True)\n",
        "\n",
        "    spark_df = spark.createDataFrame(weights_df)\n",
        "\n",
        "    # Table name and database name\n",
        "    table_name = \"Linear_Regression_Results\"\n",
        "    database_name = \"alpha_two\"\n",
        "\n",
        "    # Check if table exists\n",
        "    if spark.catalog.tableExists(f\"{database_name}.{table_name}\"):\n",
        "        # Table exists, union Spark DataFrame with existing table\n",
        "        spark_df.unionAll(spark.table(f\"{database_name}.{table_name}\")).write.mode(\"append\").saveAsTable(f\"{database_name}.{table_name}\")\n",
        "    else:\n",
        "        # Table does not exist, create table from Spark DataFrame\n",
        "        spark_df.write.saveAsTable(f\"{database_name}.{table_name}\")\n",
        "\n",
        "\n",
        "print(weights_df)\n"
      ],
      "metadata": {
        "id": "xiV7BQOaZp54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6i5bNCyZrTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}