{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDUBr0bkWXA4KPMac9fCMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuinnG17/Financial-Programs/blob/main/MODEL_ANALYSIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtYWw3x3Z3hs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read long signal return information\n",
        "long_df = spark.read.table(\"alpha_two.all_date_long_metrics_v2\")\n",
        "long_data = long_df.toPandas()\n",
        "long_data['Date'] = pd.to_datetime(long_data['Date'])\n",
        "\n",
        "# Read short signal return information\n",
        "short_df = spark.read.table(\"alpha_two.all_date_short_metrics_v2\")\n",
        "short_data = short_df.toPandas()\n",
        "short_data['Date'] = pd.to_datetime(short_data['Date'])\n",
        "\n",
        "#read Signals\n",
        "table_df = spark.read.table(\"kash.signal_shifted\")\n",
        "signals = table_df.toPandas()\n",
        "signals['Date'] = pd.to_datetime(signals['Date'])\n",
        "\n",
        "#Concat data for Target column calculation\n",
        "all_data = pd.concat([long_data, short_data])\n",
        "data = pd.merge(all_data, signals, on='Date', how='inner')\n",
        "data['Signal'] = data['Signal'].replace(2, -1)\n",
        "\n",
        "data = data.sort_values(by='Date')\n",
        "\n",
        "# Subtract the next row by the current row within each group\n",
        "data['NextValue'] = data.groupby('Symbol')['Net_Profit'].shift(-1)\n",
        "data['Target'] = data['NextValue'] - data['Net_Profit']\n",
        "\n",
        "print(long_data.columns)\n",
        "print(data.columns)\n",
        "\n",
        "#Separate Target Values back out\n",
        "long_data = long_data.merge(data[['Symbol', 'Date', 'NextValue', 'Signal', 'Target']], on=['Date', 'Symbol'], how='inner')\n",
        "short_data = short_data.merge(data[['Symbol', 'Date', 'NextValue', 'Signal', 'Target']], on=['Date','Symbol'], how='inner')\n",
        "\n",
        "long_data = long_data.sort_values('Date')\n",
        "short_data = short_data.sort_values('Date')\n",
        "\n",
        "print(long_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "for date in data['Date'].head():\n",
        "    print(date.date() - timedelta(days=60))"
      ],
      "metadata": {
        "id": "sBi86NUradj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute to analyze model performance per symbol\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from datetime import datetime, date\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "def measure_best_features(data, models, weights_df):\n",
        "    \"\"\"\n",
        "    Measure the best features for each stock at each backtest date using different feature selection methods.\n",
        "    \"\"\"\n",
        "    best_models = []\n",
        "    unique_symbols = data['Symbol'].unique()\n",
        "\n",
        "    if data['Signal'].iloc[-1] == 1:\n",
        "        direction = 'Long'\n",
        "        print(direction)\n",
        "\n",
        "    elif data['Signal'].iloc[-1] == -1:\n",
        "        direction = 'Short'\n",
        "    else:\n",
        "        direction = None\n",
        "\n",
        "    i = 0\n",
        "    for symbol in unique_symbols:\n",
        "\n",
        "        symbol_data = data[data['Symbol'] == symbol]\n",
        "\n",
        "        symbol_data = symbol_data.dropna()\n",
        "\n",
        "        if len(symbol_data) < 100:\n",
        "            continue\n",
        "\n",
        "        # Separate features and target\n",
        "        X = symbol_data.drop(['Date', 'Symbol', 'Target', 'NextValue'], axis=1)\n",
        "        #print(X)\n",
        "        y = symbol_data['Target']\n",
        "        #print(y)\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Perform feature selection using different methods\n",
        "\n",
        "        print(f\"running models @ {symbol}\")\n",
        "        for model in models:\n",
        "            print(f\"running {model}\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "            # Calculate RMSE\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "            accuracy = accuracy_above_threshold(y_val, y_pred, threshold=0)\n",
        "\n",
        "            if isinstance(model, SVR):\n",
        "                # Calculate permutation importances\n",
        "                importances = permutation_importance(model, X, y)\n",
        "\n",
        "                # Get the feature importances\n",
        "                weights = importances.importances_mean\n",
        "\n",
        "            elif isinstance(model, LinearRegression):\n",
        "                weights = model.coef_\n",
        "            else:\n",
        "                weights = model.feature_importances_\n",
        "\n",
        "            weights_df_temp = pd.DataFrame([weights], columns=X.columns.tolist())\n",
        "            weights_df_temp['Symbol'] = symbol\n",
        "            weights_df_temp['Model'] = model.__class__.__name__\n",
        "            weights_df_temp['RMSE'] = rmse / 100\n",
        "            weights_df_temp['Accuracy'] = accuracy\n",
        "            weights_df_temp['Direction'] = direction\n",
        "            weights_df = weights_df.append(weights_df_temp)\n",
        "\n",
        "        print(i / len(unique_symbols))\n",
        "        i += 1\n",
        "\n",
        "    return weights_df\n",
        "\n",
        "def accuracy_above_threshold(y_true, y_pred, threshold=0):\n",
        "    y_true_binary = y_true > threshold\n",
        "    y_pred_binary = y_pred > threshold\n",
        "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Define your list of ML estimator models\n",
        "models = [\n",
        "     RandomForestRegressor(),\n",
        "     #SVR(),\n",
        "     LinearRegression(),\n",
        "     DecisionTreeRegressor(),\n",
        "     AdaBoostRegressor(),\n",
        "     GradientBoostingRegressor(),\n",
        "     ExtraTreesRegressor()\n",
        " ]\n",
        "\n",
        "weights_df = pd.DataFrame()\n",
        "\n",
        "weights_df = measure_best_features(long_data, models, weights_df)\n",
        "weights_df = measure_best_features(short_data, models, weights_df)\n",
        "\n",
        "   # Create a SparkSession\n",
        "spark = SparkSession.builder.appName('pandas-to-databricks').getOrCreate()\n",
        "\n",
        "# Convert the Pandas DataFrame to a Databricks DataFrame\n",
        "db_df = spark.createDataFrame(weights_df)\n",
        "\n",
        "db_df.createOrReplaceTempView('model_weights')\n",
        "\n",
        "db_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TnGdkRw3aeG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute to analyze model performance per date\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from datetime import datetime, date\n",
        "from pyspark.sql import SparkSession\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "def measure_best_features(data, models, weights_df):\n",
        "    \"\"\"\n",
        "    Measure the best features for each stock at each backtest date using different feature selection methods.\n",
        "    \"\"\"\n",
        "    best_models = []\n",
        "    unique_date = data['Date'].unique()\n",
        "\n",
        "    if data['Signal'].iloc[-1] == 1:\n",
        "        direction = 'Long'\n",
        "        print(direction)\n",
        "\n",
        "    elif data['Signal'].iloc[-1] == -1:\n",
        "        direction = 'Short'\n",
        "    else:\n",
        "        direction = None\n",
        "\n",
        "    i = 0\n",
        "    for date in unique_date:\n",
        "\n",
        "        date_data = data[data['Date'] == date]\n",
        "\n",
        "        date_data = date_data.dropna()\n",
        "\n",
        "        if len(date_data) < 100:\n",
        "            continue\n",
        "\n",
        "        # Separate features and target\n",
        "        X = date_data.drop(['Date', 'Symbol', 'Target', 'NextValue'], axis=1)\n",
        "        #print(X)\n",
        "        y = date_data['Target']\n",
        "        #print(y)\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Perform feature selection using different methods\n",
        "\n",
        "        print(f\"running models @ {date}\")\n",
        "        for model in models:\n",
        "            print(f\"running {model}\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "            # Calculate RMSE\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "            accuracy = accuracy_above_threshold(y_val, y_pred, threshold=0)\n",
        "\n",
        "            if isinstance(model, SVR):\n",
        "                # Calculate permutation importances\n",
        "                importances = permutation_importance(model, X, y)\n",
        "\n",
        "                # Get the feature importances\n",
        "                weights = importances.importances_mean\n",
        "\n",
        "            elif isinstance(model, LinearRegression):\n",
        "                weights = model.coef_\n",
        "            else:\n",
        "                weights = model.feature_importances_\n",
        "\n",
        "            weights_df_temp = pd.DataFrame([weights], columns=X.columns.tolist())\n",
        "            weights_df_temp['Date'] = date\n",
        "            weights_df_temp['Model'] = model.__class__.__name__\n",
        "            weights_df_temp['RMSE'] = rmse / 100\n",
        "            weights_df_temp['Accuracy'] = accuracy\n",
        "            weights_df_temp['Direction'] = direction\n",
        "            weights_df = weights_df.append(weights_df_temp)\n",
        "\n",
        "        print(i / len(unique_date))\n",
        "        i += 1\n",
        "\n",
        "    return weights_df\n",
        "\n",
        "def accuracy_above_threshold(y_true, y_pred, threshold=0):\n",
        "    y_true_binary = y_true > threshold\n",
        "    y_pred_binary = y_pred > threshold\n",
        "    accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# Define your list of ML estimator models\n",
        "models = [\n",
        "     RandomForestRegressor(),\n",
        "     #SVR(),\n",
        "     LinearRegression(),\n",
        "     DecisionTreeRegressor(),\n",
        "     AdaBoostRegressor(),\n",
        "     GradientBoostingRegressor(),\n",
        "     ExtraTreesRegressor()\n",
        " ]\n",
        "\n",
        "weights_df = pd.DataFrame()\n",
        "\n",
        "weights_df = measure_best_features(long_data, models, weights_df)\n",
        "weights_df = measure_best_features(short_data, models, weights_df)\n",
        "\n",
        "   # Create a SparkSession\n",
        "spark = SparkSession.builder.appName('pandas-to-databricks').getOrCreate()\n",
        "\n",
        "# Convert the Pandas DataFrame to a Databricks DataFrame\n",
        "db_df = spark.createDataFrame(weights_df)\n",
        "\n",
        "db_df.createOrReplaceTempView('model_weights')\n",
        "\n",
        "db_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "zLjV5ABIafdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql create or replace table alpha_two.DAILY_MODEL_ANALYSIS as select * from model_weights"
      ],
      "metadata": {
        "id": "ylaVKsxtahQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select * from alpha_two.DAILY_MODEL_ANALYSIS where Direction = 'Long'"
      ],
      "metadata": {
        "id": "62Y9jrC8akPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.table(\"alpha_two.Regression_Model_Meta_Analysis_Combined_Next_Day\")\n",
        "\n",
        "data = data.toPandas()\n",
        "\n",
        "data = data[data['Accuracy'] != 1]\n",
        "\n",
        "# Group the data by 'Column' and calculate the average accuracy of the top decile per group\n",
        "result = data.groupby('Model').apply(lambda x: x[x['Accuracy'] >= x['Accuracy'].quantile(0.9)]['Accuracy'].mean())\n",
        "\n",
        "# Display the average accuracy per column group\n",
        "print(result)"
      ],
      "metadata": {
        "id": "D-uVxU-iao1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql select * from alpha_two.DAILY_MODEL_ANALYSIS"
      ],
      "metadata": {
        "id": "VJUKU3heawZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "dC3h7Voia-jm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}